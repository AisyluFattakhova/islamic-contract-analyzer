{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Generate Embeddings on Google Colab\n",
        "\n",
        "This notebook runs embedding generation on Google Colab's free GPU.\n",
        "\n",
        "**Steps:**\n",
        "1. Upload your dataset CSV file\n",
        "2. Install dependencies\n",
        "3. Run embedding generation\n",
        "4. Download results\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 1: Check GPU Availability\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Check if GPU is available\n",
        "import torch\n",
        "\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"âœ… GPU Available: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "    device = \"cuda\"\n",
        "else:\n",
        "    print(\"âŒ No GPU available. Please enable GPU in Runtime > Change runtime type > GPU\")\n",
        "    device = \"cpu\"\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 2: Install Dependencies\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages\n",
        "%pip install sentence-transformers pandas numpy tqdm -q\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 3: Upload Dataset\n",
        "\n",
        "Upload your `standards_dataset.csv` file using the file uploader below.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import pandas as pd\n",
        "from pathlib import Path\n",
        "import os\n",
        "\n",
        "# Create datasets directory\n",
        "os.makedirs(\"datasets\", exist_ok=True)\n",
        "os.makedirs(\"datasets/embeddings\", exist_ok=True)\n",
        "\n",
        "print(\"Please upload your standards_dataset.csv file:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Move uploaded file to datasets directory\n",
        "dataset_path = None\n",
        "for filename in uploaded.keys():\n",
        "    if filename.endswith('.csv'):\n",
        "        os.rename(filename, f\"datasets/{filename}\")\n",
        "        print(f\"âœ… Uploaded: {filename}\")\n",
        "        dataset_path = f\"datasets/{filename}\"\n",
        "        break\n",
        "\n",
        "if not dataset_path:\n",
        "    print(\"âŒ No CSV file found. Please upload standards_dataset.csv\")\n",
        "else:\n",
        "    # Verify dataset\n",
        "    df = pd.read_csv(dataset_path)\n",
        "    print(f\"\\nDataset loaded: {len(df)} sections\")\n",
        "    print(f\"Columns: {list(df.columns)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 4: Generate Embeddings\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from tqdm import tqdm\n",
        "import json\n",
        "import time\n",
        "\n",
        "if 'dataset_path' not in locals() or dataset_path is None:\n",
        "    print(\"âŒ Please upload dataset first!\")\n",
        "else:\n",
        "    # Load dataset\n",
        "    df = pd.read_csv(dataset_path)\n",
        "    texts = df['content'].tolist()\n",
        "    \n",
        "    print(f\"Total texts to embed: {len(texts):,}\")\n",
        "    \n",
        "    # Model configuration\n",
        "    model_name = \"BAAI/bge-m3\"  # High quality model\n",
        "    batch_size = 256 if device == \"cuda\" else 16\n",
        "    use_fp16 = device == \"cuda\"\n",
        "    \n",
        "    print(f\"\\nConfiguration:\")\n",
        "    print(f\"  Model: {model_name}\")\n",
        "    print(f\"  Device: {device}\")\n",
        "    print(f\"  Batch size: {batch_size}\")\n",
        "    print(f\"  FP16: {use_fp16}\")\n",
        "    \n",
        "    # Load model\n",
        "    print(f\"\\nLoading model...\")\n",
        "    model = SentenceTransformer(model_name, device=device)\n",
        "    if use_fp16:\n",
        "        model = model.half()\n",
        "        print(\"Using FP16 (half precision) for faster processing\")\n",
        "    \n",
        "    # Generate embeddings\n",
        "    print(f\"\\nGenerating embeddings...\")\n",
        "    total_batches = (len(texts) + batch_size - 1) // batch_size\n",
        "    embeddings = []\n",
        "    start_time = time.time()\n",
        "    \n",
        "    with tqdm(total=len(texts), desc=\"Processing\", unit=\"text\") as pbar:\n",
        "        for batch_idx, i in enumerate(range(0, len(texts), batch_size), 1):\n",
        "            batch = texts[i:i+batch_size]\n",
        "            \n",
        "            pbar.set_description(f\"Batch {batch_idx}/{total_batches}\")\n",
        "            \n",
        "            batch_embeddings = model.encode(\n",
        "                batch,\n",
        "                show_progress_bar=False,\n",
        "                convert_to_numpy=True,\n",
        "                normalize_embeddings=True,\n",
        "                batch_size=len(batch)\n",
        "            )\n",
        "            \n",
        "            embeddings.append(batch_embeddings)\n",
        "            pbar.update(len(batch))\n",
        "            \n",
        "            # Show progress\n",
        "            elapsed = time.time() - start_time\n",
        "            texts_processed = min(i + len(batch), len(texts))\n",
        "            texts_per_sec = texts_processed / elapsed if elapsed > 0 else 0\n",
        "            \n",
        "            if batch_idx > 0 and elapsed > 0:\n",
        "                avg_time_per_text = elapsed / texts_processed\n",
        "                remaining_texts = len(texts) - texts_processed\n",
        "                eta = remaining_texts * avg_time_per_text\n",
        "            else:\n",
        "                eta = 0\n",
        "            \n",
        "            pbar.set_postfix({\n",
        "                'Speed': f\"{texts_per_sec:.1f} texts/s\",\n",
        "                'ETA': f\"{eta:.0f}s\" if eta > 0 else \"calculating...\"\n",
        "            })\n",
        "    \n",
        "    # Concatenate embeddings\n",
        "    all_embeddings = np.vstack(embeddings)\n",
        "    total_time = time.time() - start_time\n",
        "    \n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"âœ… Embeddings generated successfully!\")\n",
        "    print(f\"{'='*60}\")\n",
        "    print(f\"Total time: {total_time:.2f} seconds ({total_time/60:.2f} minutes)\")\n",
        "    print(f\"Average speed: {len(texts)/total_time:.1f} texts/second\")\n",
        "    print(f\"Embedding shape: {all_embeddings.shape}\")\n",
        "    print(f\"Embedding dimension: {all_embeddings.shape[1]}\")\n",
        "    \n",
        "    # Save embeddings\n",
        "    embeddings_path = \"datasets/embeddings/embeddings.npy\"\n",
        "    np.save(embeddings_path, all_embeddings)\n",
        "    print(f\"\\nâœ… Saved embeddings to: {embeddings_path}\")\n",
        "    \n",
        "    # Save metadata\n",
        "    metadata = {\n",
        "        'model_name': model_name,\n",
        "        'embedding_dimension': int(all_embeddings.shape[1]),\n",
        "        'num_embeddings': int(all_embeddings.shape[0]),\n",
        "        'normalized': True,\n",
        "        'processing_time_seconds': float(total_time),\n",
        "        'texts_per_second': float(len(texts) / total_time)\n",
        "    }\n",
        "    \n",
        "    metadata_path = \"datasets/embeddings/metadata.json\"\n",
        "    with open(metadata_path, 'w') as f:\n",
        "        json.dump(metadata, f, indent=2)\n",
        "    print(f\"âœ… Saved metadata to: {metadata_path}\")\n",
        "    \n",
        "    # Update dataset with embedding indices\n",
        "    df['embedding_index'] = range(len(df))\n",
        "    output_dataset = \"datasets/standards_dataset_with_embeddings.csv\"\n",
        "    df.to_csv(output_dataset, index=False)\n",
        "    print(f\"âœ… Saved dataset with indices to: {output_dataset}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Step 5: Download Results\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "import zipfile\n",
        "import os\n",
        "\n",
        "# Create zip file with all results\n",
        "zip_path = \"embeddings_results.zip\"\n",
        "with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
        "    # Add embeddings\n",
        "    if os.path.exists(\"datasets/embeddings/embeddings.npy\"):\n",
        "        zipf.write(\"datasets/embeddings/embeddings.npy\", \"embeddings.npy\")\n",
        "    \n",
        "    # Add metadata\n",
        "    if os.path.exists(\"datasets/embeddings/metadata.json\"):\n",
        "        zipf.write(\"datasets/embeddings/metadata.json\", \"metadata.json\")\n",
        "    \n",
        "    # Add dataset with indices\n",
        "    if os.path.exists(\"datasets/standards_dataset_with_embeddings.csv\"):\n",
        "        zipf.write(\"datasets/standards_dataset_with_embeddings.csv\", \"standards_dataset_with_embeddings.csv\")\n",
        "\n",
        "print(\"ðŸ“¦ Created zip file with all results\")\n",
        "print(\"\\nDownloading files...\")\n",
        "files.download(zip_path)\n",
        "print(\"\\nâœ… Download complete!\")\n",
        "print(\"\\nFiles included:\")\n",
        "print(\"  - embeddings.npy (embeddings array)\")\n",
        "print(\"  - metadata.json (model info)\")\n",
        "print(\"  - standards_dataset_with_embeddings.csv (dataset with indices)\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
